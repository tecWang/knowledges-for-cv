
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [常见的集成学习算法并简述](#常见的集成学习算法并简述)
- [决策树如何进行回归与分类](#决策树如何进行回归与分类)
  - [决策树的主要种类](#决策树的主要种类)
  - [为什么CART(Classification and Regression Tree)可以做回归，而ID3和C4.5不可以](#为什么cartclassification-and-regression-tree可以做回归而id3和c45不可以)
  - [决策树的优缺点](#决策树的优缺点)
  - [决策树与随机森林](#决策树与随机森林)
    - [Bagging算法简述](#bagging算法简述)
    - [随机森林算法概述](#随机森林算法概述)
    - [头条面试题：随机森林的随机怎么理解？](#头条面试题随机森林的随机怎么理解answers-listmd头条面试题1)
- [References](#references)

<!-- /code_chunk_output -->

### 常见的集成学习算法并简述

集成学习有两个流派，一个是boosting派系，它的特点是各个弱学习器之间有依赖关系。另一种是bagging流派，它的特点是各个弱学习器之间没有依赖关系，可以并行拟合。

Boosting系列的主要算法为Adaboost和GBDT。

Bagging系列的主要算法为RandomForest。

### 决策树如何进行回归与分类

#### 决策树的主要种类

ID3, C4.5, CART

其中ID3主要是通过信息熵和信息增益进行选择最优属性进行分裂， C4.5主要通过信息增益比进行分裂，CART主要通过基尼系数进行分裂。

- ID3算法。(a) ID3没有考虑连续值，如长度、密度等；(b) ID3没有对缺失值进行考虑；(c) 没有考虑过拟合问题；(d) 在相同条件下取值较多的特征，比取值较小的特征的信息增益大。
$$\begin{aligned}
H(D) & = -\sum_{i=1}^{n}{P_i\log{P_i}} \\
g(D, A) & = H(D) - H(D|A) \\ 
        & = H(D) - \sum_{i=1}^{n}{P_i H(D|A_i)}
\end{aligned}$$其中，$P_i$为当前属性下不同取值的概率，$H(D|A_i)$为当前数据下下，当前属性值时，不同类别的信息熵。

- C4.5算法。解决了ID3的 (d) 问题，特征数越多的特征对应的信息熵越大，应作为分母以矫正信息增益的偏向的问题。

$$I_R{(D, A)} = \frac{g(D, A)}{H(D)}$$

#### 为什么CART(Classification and Regression Tree)可以做回归，而ID3和C4.5不可以

回归决策树主要指CART算法，内部结点特征的取值为“是”和“否”， 为二叉树结构。所谓回归，就是根据特征向量来决定对应的输出值。回归树就是将特征空间划分成若干单元，每一个划分单元有一个特定的输出。因为每个结点都是“是”和“否”的判断，所以划分的边界是平行于坐标轴的。对于测试数据，我们只要按照特征将其归到某个单元，便得到对应的输出值。划分的过程也就是建立树的过程，每划分一次，随即确定划分单元对应的输出，也就多了一个结点。当根据停止条件划分终止的时候，最终每个单元的输出也就确定了，也就是叶结点。

CART可以做回归，而ID3和C4.5不可以回归是因为几个模型的损失函数及分裂方式都不同。

- 分裂方式。CART有两种评价标准：Variance和Gini系数。而ID3和C4.5的评价基础都是信息熵。信息熵和Gini系数是针对分类任务的指标，而**Variance是针对连续值的指标因此可以用来做回归**。

- 损失函数。**对于CART回归模型来讲**，其回归模型采用和方差的形式度量划分特征A的优劣。度量目标是对于划分特征A， 对应的划分点S两边的数据集D1,D2，求出使D1，D2各自集合的均方差最小，同时使D1和D2的均方差之和最小。
$$L = \underbrace{min}_{A, S}[\underbrace{min}_{c1} \sum_{x_i \in D_1(A, S)}{(y_i - c_1)^2} + \underbrace{min}_{c_2} \sum_{x_i \in D_2(A, S)}{(y_i - c_2)^2}]$$**对于分类模型来讲**，采用基尼指数或信息熵等评价指标度量各个划分点的优劣。

#### 决策树的优缺点

决策树的一些优点是：
- 易于理解和解释。决策树可以进行可视化。
- 需要很少的数据准备。其他技术通常需要数据规范化，需要创建伪变量并删除空白值。但是请注意，此模块不支持缺少的值。
- 使用树的成本（即预测数据）与用于训练树的数据点数量成对数。
- 能够处理数字和分类数据。其他技术通常专用于分析仅具有一种类型的变量的数据集。有关更多信息，请参见算法。
- 能够处理**多输出问题**。
- 使用白盒模型。如果模型中可以观察到给定的情况，则可以通过布尔逻辑轻松解释条件。相反，在黑匣子模型中（例如，在人工神经网络中），结果可能更难以解释。
- 可以使用统计测试来验证模型。这使得考虑模型的可靠性成为可能。
- **即使生成数据的真实模型在某种程度上违背了它的假设，也可以表现良好**。

决策树的缺点包括：
- **决策树学习者可能会创建过于复杂的树，从而无法很好地概括数据。这称为过度拟合**。为避免此问题，必须使用诸如剪枝、设置叶节点处所需的最小样本数或设置树的最大深度之类的机制。
- **决策树可能不稳定**，因为数据中的细微变化可能会导致生成完全不同的树。**通过使用集成学习中的决策树可以缓解此问题**。
- 在最优性的几个方面，甚至对于简单的概念，学习最优决策树的问题都被认为是NP完全的。因此，实用的决策树学习算法基于启发式算法（例如贪婪算法），其中在每个节点上做出局部最优决策。这样的算法不能保证返回全局最优决策树。可以通过在集成学习器中训练多棵树来缓解这种情况，在该学习器中，特征和样本将通过替换随机抽样。
- 有些概念很难学习，因为决策树无法轻松表达它们，例如XOR，奇偶校验或多路复用器问题。
- **类别不均衡的处理能力较差，如果某些类别占主导地位，则决策树学习者会创建有偏见的树**。因此，建议在与决策树拟合之前平衡数据集

#### 决策树与随机森林

##### Bagging算法简述

相对于Boosting系列的Adaboost和GBDT，bagging算法要简单的多。

输入为样本集$T = \{ (x_1, y_1), (x_2, y_2), ..., (x_n, y_n) \}$，弱学习器迭代次数为K。算法流程如下:

循环训练弱分类器K次，$t=1, 2, ..., k$
- 对训练集进行第t次随机采样，共采集n次，得到包含n个样本的采样集$D_t$
- 基于采样集$D_t$训练第t个弱学习器

如果是**分类算法预测**，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是**回归算法预测**，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。

![image](./images/Bagging_theory.png)

##### 随机森林算法概述

RF较普通的bagging算法有了两处改进。第一，**RF使用了CART决策树作为弱学习器**，这让我们想到了梯度提升树GBDT。第二，在使用决策树的基础上，**RF对决策树的建立做了改进**，对于普通的决策树，我们会在节点上所有的n个样本特征中选择一个最优的特征来做决策树的左右子树划分，但是**RF通过随机选择节点上的一部分样本特征**，这个数字小于n，假设为$n_{sub}$，然后在这些随机选择的$n_{sub}$个样本特征中，选择一个最优的特征来做决策树的左右子树划分。这样进一步增强了模型的泛化能力。　　　　

如果$n_{sub}=n$，则此时RF的CART决策树和普通的CART决策树没有区别。$n_{sub}$越小，则模型约健壮，当然此时对于训练集的拟合程度会变差。也就是说$n_{sub}$越小，模型的方差会减小，但是偏倚会增大。在实际案例中，一般会通过交叉验证调参获取一个合适的$n_{sub}$的值。

常规的随机森林算法的优缺点:
RF的主要优点有：
- 训练可以高度并行化，对于大数据时代的大样本训练速度有优势。个人觉得这是的最主要的优点。
- 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。
- 在训练后，可以给出各个特征对于输出的重要性
- 由于采用了随机采样，训练出的模型的方差小，泛化能力强。
- 相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。
- 对部分特征缺失不敏感。

RF的主要缺点有：
- 在某些噪音比较大的样本集上，RF模型容易陷入过拟合。
- 取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。

##### [头条面试题：随机森林的随机怎么理解？](Answers-list.md#头条面试题1)


### References

1. [决策树之分类树与回归树](https://blog.csdn.net/qq_42451635/article/details/82146067)
1. [决策树算法原理(CART分类树)](https://www.cnblogs.com/keye/p/10564914.html)
1. [完整决策树（回归树）推导加讲解](https://zhuanlan.zhihu.com/p/79840239)
1. [为什么CART能做回归而ID3和C4.5不可以？](https://www.zhihu.com/question/299719792)
1. [Bagging与随机森林算法原理小结](https://www.cnblogs.com/pinard/p/6156009.html)